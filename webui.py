# coding=utf-8

import os
import librosa
import base64
import io
import gradio as gr
import re
import locale

import numpy as np
import torch
import torchaudio


from funasr import AutoModel

# Auto detect device: CUDA (NVIDIA) -> MPS (Apple Silicon) -> CPU
device = "cuda:0" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
print(f"Using device: {device}")

model = "iic/SenseVoiceSmall"
model = AutoModel(model=model,
		  vad_model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
		  vad_kwargs={"max_single_segment_time": 30000},
		  trust_remote_code=True,
		  device=device
		  )

import re

emo_dict = {
	"<|HAPPY|>": "ğŸ˜Š",
	"<|SAD|>": "ğŸ˜”",
	"<|ANGRY|>": "ğŸ˜¡",
	"<|NEUTRAL|>": "",
	"<|FEARFUL|>": "ğŸ˜°",
	"<|DISGUSTED|>": "ğŸ¤¢",
	"<|SURPRISED|>": "ğŸ˜®",
}

event_dict = {
	"<|BGM|>": "ğŸ¼",
	"<|Speech|>": "",
	"<|Applause|>": "ğŸ‘",
	"<|Laughter|>": "ğŸ˜€",
	"<|Cry|>": "ğŸ˜­",
	"<|Sneeze|>": "ğŸ¤§",
	"<|Breath|>": "",
	"<|Cough|>": "ğŸ¤§",
}

emoji_dict = {
	"<|nospeech|><|Event_UNK|>": "â“",
	"<|zh|>": "",
	"<|en|>": "",
	"<|yue|>": "",
	"<|ja|>": "",
	"<|ko|>": "",
	"<|nospeech|>": "",
	"<|HAPPY|>": "ğŸ˜Š",
	"<|SAD|>": "ğŸ˜”",
	"<|ANGRY|>": "ğŸ˜¡",
	"<|NEUTRAL|>": "",
	"<|BGM|>": "ğŸ¼",
	"<|Speech|>": "",
	"<|Applause|>": "ğŸ‘",
	"<|Laughter|>": "ğŸ˜€",
	"<|FEARFUL|>": "ğŸ˜°",
	"<|DISGUSTED|>": "ğŸ¤¢",
	"<|SURPRISED|>": "ğŸ˜®",
	"<|Cry|>": "ğŸ˜­",
	"<|EMO_UNKNOWN|>": "",
	"<|Sneeze|>": "ğŸ¤§",
	"<|Breath|>": "",
	"<|Cough|>": "ğŸ˜·",
	"<|Sing|>": "",
	"<|Speech_Noise|>": "",
	"<|withitn|>": "",
	"<|woitn|>": "",
	"<|GBG|>": "",
	"<|Event_UNK|>": "",
}

lang_dict =  {
    "<|zh|>": "<|lang|>",
    "<|en|>": "<|lang|>",
    "<|yue|>": "<|lang|>",
    "<|ja|>": "<|lang|>",
    "<|ko|>": "<|lang|>",
    "<|nospeech|>": "<|lang|>",
}

# ä¸­è‹±æ–‡ç¿»è¯‘å­—å…¸
translations = {
    "en": {
        "html_title": "Voice Understanding Model: SenseVoice-Small",
        "html_desc": "SenseVoice-Small is an encoder-only speech foundation model designed for rapid voice understanding. It encompasses a variety of features including automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and acoustic event detection (AED). SenseVoice-Small supports multilingual recognition for Chinese, English, Cantonese, Japanese, and Korean. Additionally, it offers exceptionally low inference latency, performing 7 times faster than Whisper-small and 17 times faster than Whisper-large.",
        "html_usage": "Usage",
        "html_usage_desc": "Upload an audio file or input through a microphone, then select the task and language. the audio is transcribed into corresponding text along with associated emotions (ğŸ˜Š happy, ğŸ˜¡ angry/exicting, ğŸ˜” sad) and types of sound events (ğŸ˜€ laughter, ğŸ¼ music, ğŸ‘ applause, ğŸ¤§ cough&sneeze, ğŸ˜­ cry). The event labels are placed in the front of the text and the emotion are in the back of the text.",
        "html_usage_recommend": "Recommended audio input duration is below 30 seconds. For audio longer than 30 seconds, local deployment is recommended.",
        "html_repo": "Repo",
        "html_sensevoice": "SenseVoice: multilingual speech understanding model",
        "html_funasr": "FunASR: fundamental speech recognition toolkit",
        "html_cosyvoice": "CosyVoice: high-quality multilingual TTS model",
        "audio_label": "Upload audio or use the microphone",
        "config_title": "Configuration",
        "language_label": "Language",
        "language_auto": "auto",
        "language_zh": "zh",
        "language_en": "en",
        "language_yue": "yue",
        "language_ja": "ja",
        "language_ko": "ko",
        "language_nospeech": "nospeech",
        "start_button": "Start",
        "results_label": "Results",
        "copy_button": "Copy Results",
        "download_button": "Download Results",
        "download_filename": "sensevoice_results.txt"
    },
    "zh": {
        "html_title": "è¯­éŸ³ç†è§£æ¨¡å‹: SenseVoice-Small",
        "html_desc": "SenseVoice-Smallæ˜¯ä¸€ä¸ªä»…ä½¿ç”¨ç¼–ç å™¨çš„è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼Œä¸“ä¸ºå¿«é€Ÿè¯­éŸ³ç†è§£è€Œè®¾è®¡ã€‚å®ƒåŒ…å«å¤šç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€å£è¯­è¯­è¨€è¯†åˆ«ï¼ˆLIDï¼‰ã€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å’Œå£°å­¦äº‹ä»¶æ£€æµ‹ï¼ˆAEDï¼‰ã€‚SenseVoice-Smallæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ã€ç²¤è¯­ã€æ—¥è¯­å’ŒéŸ©è¯­çš„å¤šè¯­è¨€è¯†åˆ«ã€‚æ­¤å¤–ï¼Œå®ƒå…·æœ‰æä½çš„æ¨ç†å»¶è¿Ÿï¼Œæ¯”Whisper-smallå¿«7å€ï¼Œæ¯”Whisper-largeå¿«17å€ã€‚",
        "html_usage": "ä½¿ç”¨æ–¹æ³•",
        "html_usage_desc": "ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶æˆ–é€šè¿‡éº¦å…‹é£è¾“å…¥ï¼Œç„¶åé€‰æ‹©ä»»åŠ¡å’Œè¯­è¨€ã€‚éŸ³é¢‘å°†è¢«è½¬å½•ä¸ºç›¸åº”çš„æ–‡æœ¬ï¼Œå¹¶å¸¦æœ‰ç›¸å…³çš„æƒ…æ„Ÿ (ğŸ˜Š happy, ğŸ˜¡ angry/exicting, ğŸ˜” sad) and types of sound events (ğŸ˜€ laughter, ğŸ¼ music, ğŸ‘ applause, ğŸ¤§ cough&sneeze, ğŸ˜­ cry)ã€‚äº‹ä»¶æ ‡ç­¾ä½äºæ–‡æœ¬å‰é¢ï¼Œæƒ…æ„Ÿæ ‡ç­¾ä½äºæ–‡æœ¬åé¢ã€‚",
        "html_usage_recommend": "å»ºè®®éŸ³é¢‘è¾“å…¥æ—¶é•¿åœ¨30ç§’ä»¥å†…ã€‚å¯¹äºè¶…è¿‡30ç§’çš„éŸ³é¢‘ï¼Œå»ºè®®æœ¬åœ°éƒ¨ç½²ã€‚",
        "html_repo": "é¡¹ç›®ä»“åº“",
        "html_sensevoice": "SenseVoice: å¤šè¯­è¨€è¯­éŸ³ç†è§£æ¨¡å‹",
        "html_funasr": "FunASR: åŸºç¡€è¯­éŸ³è¯†åˆ«å·¥å…·åŒ…",
        "html_cosyvoice": "CosyVoice: é«˜è´¨é‡å¤šè¯­è¨€TTSæ¨¡å‹",
        "audio_label": "ä¸Šä¼ éŸ³é¢‘æˆ–ä½¿ç”¨éº¦å…‹é£",
        "config_title": "é…ç½®",
        "language_label": "è¯­è¨€",
        "language_auto": "auto",
        "language_zh": "zh",
        "language_en": "en",
        "language_yue": "yue",
        "language_ja": "ja",
        "language_ko": "ko",
        "language_nospeech": "nospeech",
        "start_button": "å¼€å§‹",
        "results_label": "ç»“æœ",
        "copy_button": "å¤åˆ¶ç»“æœ",
        "download_button": "ä¸‹è½½ç»“æœ",
        "download_filename": "sensevoice_ç»“æœ.txt"
    }
}

emo_set = {"ğŸ˜Š", "ğŸ˜”", "ğŸ˜¡", "ğŸ˜°", "ğŸ¤¢", "ğŸ˜®"}
event_set = {"ğŸ¼", "ğŸ‘", "ğŸ˜€", "ğŸ˜­", "ğŸ¤§", "ğŸ˜·",}

def format_str(s):
	for sptk in emoji_dict:
		s = s.replace(sptk, emoji_dict[sptk])
	return s


def format_str_v2(s):
	sptk_dict = {}
	for sptk in emoji_dict:
		sptk_dict[sptk] = s.count(sptk)
		s = s.replace(sptk, "")
	emo = "<|NEUTRAL|>"
	for e in emo_dict:
		if sptk_dict[e] > sptk_dict[emo]:
			emo = e
	for e in event_dict:
		if sptk_dict[e] > 0:
			s = event_dict[e] + s
	s = s + emo_dict[emo]

	for emoji in emo_set.union(event_set):
		s = s.replace(" " + emoji, emoji)
		s = s.replace(emoji + " ", emoji)
	return s.strip()

def format_str_v3(s):
	def get_emo(s):
		return s[-1] if s[-1] in emo_set else None
	def get_event(s):
		return s[0] if s[0] in event_set else None

	s = s.replace("<|nospeech|><|Event_UNK|>", "â“")
	for lang in lang_dict:
		s = s.replace(lang, "<|lang|>")
	s_list = [format_str_v2(s_i).strip(" ") for s_i in s.split("<|lang|>")]
	new_s = " " + s_list[0]
	cur_ent_event = get_event(new_s)
	for i in range(1, len(s_list)):
		if len(s_list[i]) == 0:
			continue
		if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:
			s_list[i] = s_list[i][1:]
		#else:
		cur_ent_event = get_event(s_list[i])
		if get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):
			new_s = new_s[:-1]
		new_s += s_list[i].strip().lstrip()
	new_s = new_s.replace("The.", " ")
	return new_s.strip()

def model_inference(input_wav, language, fs=16000):
	# task_abbr = {"Speech Recognition": "ASR", "Rich Text Transcription": ("ASR", "AED", "SER")}
	language_abbr = {"auto": "auto", "zh": "zh", "en": "en", "yue": "yue", "ja": "ja", "ko": "ko",
					 "nospeech": "nospeech"}
	
	# task = "Speech Recognition" if task is None else task
	language = "auto" if len(language) < 1 else language
	selected_language = language_abbr[language]
	# selected_task = task_abbr.get(task)
	
	# print(f"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}")
	
	if isinstance(input_wav, tuple):
		fs, input_wav = input_wav
		input_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max
		if len(input_wav.shape) > 1:
			input_wav = input_wav.mean(-1)
		if fs != 16000:
			print(f"audio_fs: {fs}")
			resampler = torchaudio.transforms.Resample(fs, 16000)
			input_wav_t = torch.from_numpy(input_wav).to(torch.float32)
			input_wav = resampler(input_wav_t[None, :])[0, :].numpy()

	# Create a list to store logs
	logs = []

	merge_vad = True #False if selected_task == "ASR" else True
	log_msg = f"language: {language}, merge_vad: {merge_vad}"
	print(log_msg)
	# logs.append(log_msg)
	
	text = model.generate(input=input_wav,
					  cache={},
					  language=language,
					  use_itn=True,
					  batch_size_s=60, merge_vad=merge_vad)
	
	log_msg = str(text)
	print(log_msg)
	logs.append(log_msg)
	
	text = text[0]["text"]
	formatted_text = format_str_v3(text)
	
	log_msg = formatted_text
	print(log_msg)
	# logs.append(log_msg)
	
	# Join logs with newlines
	log_output = "\n\n".join(logs)
	
	return formatted_text, log_output


audio_examples = [
    ["example/zh.mp3", "zh"],
    ["example/yue.mp3", "yue"],
    ["example/en.mp3", "en"],
    ["example/ja.mp3", "ja"],
    ["example/ko.mp3", "ko"],
    # ["example/emo_1.wav", "auto"],
    # ["example/emo_2.wav", "auto"],
    # ["example/emo_3.wav", "auto"],
    #["example/emo_4.wav", "auto"],
    #["example/event_1.wav", "auto"],
    #["example/event_2.wav", "auto"],
    #["example/event_3.wav", "auto"],
    # ["example/rich_1.wav", "auto"],
    # ["example/rich_2.wav", "auto"],
    #["example/rich_3.wav", "auto"],
    # ["example/longwav_1.wav", "auto"],
    # ["example/longwav_2.wav", "auto"],
    # ["example/longwav_3.wav", "auto"],
    #["example/longwav_4.wav", "auto"],
]



def generate_html_content(lang):
    # æ ¹æ®è¯­è¨€ç”ŸæˆHTMLå†…å®¹
    t = translations[lang]
    return f"""
<div>
    <h2 style="font-size: 22px;margin-left: 0px;">{t['html_title']}</h2>
    <p style="font-size: 18px;margin-left: 20px;">{t['html_desc']}</p>
    <h2 style="font-size: 22px;margin-left: 0px;">{t['html_usage']}</h2> <p style="font-size: 18px;margin-left: 20px;">{t['html_usage_desc']}</p>
	<p style="font-size: 18px;margin-left: 20px;">{t['html_usage_recommend']}</p>
	<h2 style="font-size: 22px;margin-left: 0px;">{t['html_repo']}</h2>
	<p style="font-size: 18px;margin-left: 20px;"><a href="https://github.com/FunAudioLLM/SenseVoice" target="_blank">SenseVoice</a>: {t['html_sensevoice'].split(': ')[1]}</p>
	<p style="font-size: 18px;margin-left: 20px;"><a href="https://github.com/modelscope/FunASR" target="_blank">FunASR</a>: {t['html_funasr'].split(': ')[1]}</p>
	<p style="font-size: 18px;margin-left: 20px;"><a href="https://github.com/FunAudioLLM/CosyVoice" target="_blank">CosyVoice</a>: {t['html_cosyvoice'].split(': ')[1]}</p>
</div>
"""


def launch():
	# æ£€æµ‹ç³»ç»Ÿè¯­è¨€ç¯å¢ƒ
	lang = None
	try:
		lang, _ = locale.getlocale()
		# æ£€æµ‹ä¸­æ–‡è¯­è¨€ç¯å¢ƒï¼Œæ”¯æŒä¸åŒæ ¼å¼ï¼šzh_CNã€Chinese (Simplified)_Chinaç­‰
		current_lang = 'zh' if lang and ('zh' in lang.lower() or 'chinese' in lang.lower()) else 'en'
	except Exception as e:
		# å¦‚æœè·å–è¯­è¨€ç¯å¢ƒå¤±è´¥ï¼Œé»˜è®¤ä½¿ç”¨è‹±æ–‡
		print(f"Failed to get locale: {e}")
		current_lang = 'en'
	

	print(f"lang: {lang}")
	print(f"current_lang: {current_lang}")

	
	with gr.Blocks() as demo:
		# gr.Markdown(description)
		gr.HTML(generate_html_content(current_lang))
		with gr.Row():
			with gr.Column():
				t = translations[current_lang]
				audio_inputs = gr.Audio(label=t['audio_label'])
				
				with gr.Accordion(t['config_title']):
					language_inputs = gr.Dropdown(
					value="auto",
					label=t['language_label'],
					# åŠ¨æ€è®¾ç½®ä¸‹æ‹‰é€‰é¡¹çš„æ˜¾ç¤ºæ–‡æœ¬å’Œå®é™…å€¼
					choices=[
						(t['language_auto'], "auto"),
						(t['language_zh'], "zh"),
						(t['language_en'], "en"),
						(t['language_yue'], "yue"),
						(t['language_ja'], "ja"),
						(t['language_ko'], "ko"),
						(t['language_nospeech'], "nospeech")
					]
				)
				fn_button = gr.Button(t['start_button'], variant="primary")
				text_outputs = gr.Textbox(label=t['results_label'], lines=10, max_lines=20, scale=2)
				with gr.Row():
					copy_button = gr.Button(t['copy_button'], variant="secondary")
					download_button = gr.Button(t['download_button'], variant="secondary")
			
				# Add copy functionality using JavaScript (lightweight without alert)
				copy_button.click(
					None,
					inputs=[text_outputs],
					outputs=None,
					js="""
					async (text) => {
						if (text) {
							await navigator.clipboard.writeText(text);
						}
					}
					"""
				)
				
				# Add download functionality using JavaScript (direct download)
				download_button.click(
					None,
					inputs=[text_outputs],
					outputs=None,
					js=f"""
					(text) => {{
						if (text) {{
							const blob = new Blob([text], {{ type: 'text/plain;charset=utf-8' }});
							const url = URL.createObjectURL(blob);
							const a = document.createElement('a');
							a.href = url;
							a.download = '{t["download_filename"]}';
							document.body.appendChild(a);
							a.click();
							document.body.removeChild(a);
							URL.revokeObjectURL(url);
						}}
					}}
					"""
				)
				
				# Add log display component
				log_outputs = gr.Textbox(label="Logs", lines=5, max_lines=20, interactive=False)
			
			gr.Examples(examples=audio_examples, inputs=[audio_inputs, language_inputs], examples_per_page=20)
		
		# Update button click to handle multiple outputs
		fn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=[text_outputs, log_outputs])

	demo.launch()


if __name__ == "__main__":
	# iface.launch()
	launch()


